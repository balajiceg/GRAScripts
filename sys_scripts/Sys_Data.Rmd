---
title: "Syndromic surveillance analysis"
output: html_notebook
---
tropical strom imelda http://floodobservatory.colorado.edu/Events/4797/2019USA4797.html  
<!--

#### Load packages
```{r Load packages, message=FALSE}
library(GISTools)
library(rgdal)
library(dplyr)
library(plotly)
library(zoo)
#library(zipcodeR)
library(ggplot2)
library(sf)
library(readxl)
library(plyr)
library(knitr)
library(hrbrthemes)
```


```{r echo=FALSE}
#data("zip_code_db")
#zipcode and state
#zip_st<-zip_code_db[,c('zipcode','state')]

#tropical strom imelda http://floodobservatory.colorado.edu/Events/4797/2019USA4797.html
```


#### Read sys data and link cross wak zctas
```{r}
sys_raw<-read.csv("Z:\\Balaji\\SyS data\\merged_with_rowid.csv")
#remove the text diagnosits fields
sys_raw<-subset(sys_raw,select=-c(ChiefComplaintOrig,Discharge.Diagnosis,ProviderDiagnosis))
recs_raw<-dim(sys_raw)[1]
```
Total records =>  **`r recs_raw`**  


#### Read udsa mapper cross walk file and map zcta
```{r}
zip_zcta_crosswalk<-read_excel('Z:\\Balaji\\Census_data_texas\\Crosswalk\\Zip_to_zcta_crosswalk_2020.xlsx')
sys_raw$zip_5c<-substr(sys_raw$Zipcode,1,5)
sys_raw<-merge(sys_raw, zip_zcta_crosswalk[,c('ZIP_CODE','ZCTA')],by.x='zip_5c',by.y='ZIP_CODE',all.y=F,all.x=T)
colnames(sys_raw)[colnames(sys_raw)=='ZCTA']<-'crossed_zcta'
```


```{r echo=FALSE}
####  Find the how many records in how many states
# sys_raw<-merge(sys_raw,zip_st,by.x='zip_5c',by.y='zipcode',all.y=FALSE,all.x=TRUE)
# a<-data.frame(table(sys_raw$state))
#98.86% records from TX, 0.3% from LA
```

```{r echo=False}
####  Map of tracts and ed counts using both direct and crosswalk ----
# zctas_all<-readOGR(dsn='Z:\\Balaji\\Census_data_texas\\tl_2019_us_zcta510',layer = 'tl_2019_us_zcta510')
# #counts as per mapped zctas
# counts_df_zcta<-sys_raw %>% group_by(crossed_zcta) %>% summarise(count_zcta=n())
# #counts as per directs zips
# counts_df_zips<-sys_raw %>% group_by(zip_5c) %>% summarise(count_zips=n())
# 
# #merge counts to zctas shp
# zctas_all_merg<-merge(zctas_all,counts_df_zcta,by.x='ZCTA5CE10', by.y='crossed_zcta',all.x=T )
# zctas_all_merg<-merge(zctas_all_merg,counts_df_zips,by.x='ZCTA5CE10', by.y='zip_5c',all.x=T)
# 
# #drop the unused boundries
# zctas_all_merg<-zctas_all_merg[((!is.na(zctas_all_merg$count_zcta)) | (!is.na(zctas_all_merg$count_zips))),]
# 
# #extract texass alone
# texas_bound<-readOGR(dsn="Z:\\Balaji\\Census_data_texas\\texas_boundry_extrc_census",layer = 'texas_bound_census')
# intersections<-st_intersects(st_as_sf(zctas_all_merg),st_as_sf(texas_bound))
# zctas_txla<-zctas_all_merg[as.logical(unlist(lapply(intersections,length))),]
# 
# #subset boundries with ed counts > 10
# zctas_txla<-subset(zctas_txla,count_zcta>10)
# #calcuate diff between direct zip code linking and through croww walk
# zctas_txla$DIFF<-zctas_txla$count_zcta - zctas_txla$count_zips
# mapView(zctas_txla,zcol='DIFF')
# 
```

#### Find zctas in texas alone for filtering the sys records 
```{r}
#zctas_all<-readOGR(dsn='Z:\\Balaji\\Census_data_texas\\tl_2019_us_zcta510',layer = 'tl_2019_us_zcta510')
#texas_bound<-readOGR(dsn="Z:\\Balaji\\Census_data_texas\\texas_boundry_extrc_census",layer = 'texas_bound_census')
#check intersection
#intersections<-st_intersects(st_as_sf(zctas_all),st_as_sf(texas_bound))
#zctas_txla<-zctas_all$ZCTA5CE10[as.logical(unlist(lapply(intersections,length)))]
#write this to file so as not to repeat
#write.csv(zctas_txla,"Z:\\Balaji\\Census_data_texas\\Zctas_in_texas_list\\zctzs_texas.csv",row.names = F)
zctas_txla<-as.list(read.csv("Z:\\Balaji\\Census_data_texas\\Zctas_in_texas_list\\zctzs_texas.csv"))[[1]]

```
**`r length(zctas_txla)`** zctas selected by this


####  Read zcta inundatiaon
```{r warning=FALSE}
# read and plot percentiles of flood ratio
inun_zcta<-readOGR(dsn='Z:\\Balaji\\FloodInun_zcta_imelda_v2',layer = 'FloodInun_zcta_imelda')
spplot(inun_zcta,zcol='ZCT_f_R')
#round off to 3 decimal points
#inun_zcta@data$ZCT_f_R<-round(inun_zcta@data$ZCT_f_R,3)
#plot
plot(seq(0,1,by=0.1),quantile(inun_zcta$ZCT_f_R,seq(0,1,by=0.1)),type='b',ylab='flood Ratio',xlab='percentile')
axis(side=1, at=seq(0,1,by=0.1))
```
Decile values: `r quantile(inun_zcta$ZCT_f_R,seq(0,1,by=0.1))`

Total number of Zctas:
**`r dim(inun_zcta)[1]`**  
Number of zctas <1% flooded:
**`r dim(inun_zcta[inun_zcta$ZCT_f_R<0.01,])[1]`**  
Number of zctas >0% flooded:
**`r dim(inun_zcta[inun_zcta$ZCT_f_R>0.00,])[1]`**



####  Filter records in this texas (not study area) alone and with date after 2019
```{r}
sys_sa<-sys_raw[sys_raw$crossed_zcta %in% zctas_txla ,]
sys_sa$Date<-as.Date(sys_sa$Date, format= "%m/%d/%Y")
sys_sa<-sys_sa[sys_sa$Date > '2018-12-31',]
recs_tx<-dim(sys_sa)[1]
```
Total records: **`r recs_tx`**

####  Join syndromic surveillence with flood map
```{r}
sys_sa<-merge(sys_sa, inun_zcta@data[,c('ZCTA5CE10','ZCT_f_R')], by.x='crossed_zcta', by.y='ZCTA5CE10', all.x=TRUE)
#filter records in study arean - 
sys_sa<-sys_sa[!is.na(sys_sa$ZCT_f_R),]
recs_sa<-dim(sys_sa)[1]
```
Records in study area = **`r recs_sa`**
Length unique ZTCA = **`r length(unique(sys_sa$crossed_zcta))`** , so there are atleaset one record in each zcta


#### Format variables
```{r}
#Age - merge O and U to unknownRa
sys_sa$Sex [!(sys_sa$Sex %in% c('M','F'))] <- 'Unknown'
sys_sa$Sex <- factor(sys_sa$Sex, levels = c("M","F",'Unknown'))

#Ethnicity 
sys_sa$Ethnicity[sys_sa$Ethnicity=="2135-2"]<- "HISPANIC"  #hispanic or latino code to hispanic
sys_sa$Ethnicity[sys_sa$Ethnicity=="2186-5"]<- "NON HISPANIC"   #not hispanic or latino code to non hispanic
# Unknown -> 1, 2, 2161-8 (Salvadoran), 2178-2 (Latin American), 3, 4, 48039, 48201, ASKU, NR, Refused, UNK  
sys_sa$Ethnicity<-mapvalues(sys_sa$Ethnicity,c("1", "2", "2161-8", "2178-2", "3", "4","48039","48201","ASKU", "NR", "Refused", "UNK"), rep('Unknown',12))
sys_sa$Ethnicity<-factor(sys_sa$Ethnicity,levels = c("NON HISPANIC", "HISPANIC", "Unknown"))

#Race
sys_sa$Race[sys_sa$Race %in% c( "1002-5",  "AI", "American Indian or Alaska Native", "AMIN" )] <- "American Indian"
sys_sa$Race[sys_sa$Race %in% c( "Asian", "2028-9" )] <- "Asian"
sys_sa$Race[sys_sa$Race %in% c( "2054-5", "B", "Black or African American" )] <- "Black"
sys_sa$Race[sys_sa$Race %in% c( "2106-3", "White", "W" )] <- "White"
sys_sa$Race[sys_sa$Race %in% c( "2076-8", "2079-2", "Native Hawaiian or Other Pacific Islander", "H", "Hawaiian" )] <- "Hawaiian"
sys_sa$Race[sys_sa$Race %in% c( "2118-8", "2129-5", "2131-1", "Other", "Other Race" )] <- "Others"
sys_sa$Race[sys_sa$Race %in% c( "A",  "ASKU",  "DECLINED",  "M",  "NR", "O", "T", "UNK",  "X" )] <- "Unknown"
sys_sa$Race<-factor(sys_sa$Race,levels = c("White","Black","Asian","American Indian","Hawaiian","Others","Unknown"))
summary(sys_sa[,c('Sex','Ethnicity','Race')])

```

#### add week day and month
```{r}
sys_sa$weekday<-as.factor(weekdays(sys_sa$Date,abbreviate=T))
sys_sa$month<-as.factor(months(sys_sa$Date,abbreviate = T))
sys_sa$day<-as.factor(format(sys_sa$Date, "%d"))
```


#### Group by date and create counts column
```{r}

ZCTAdaily_count<-sys_sa %>% group_by(crossed_zcta,Date) %>% dplyr::summarise(ZCTAdaily_count=n())
sys_sa<-merge(sys_sa,ZCTAdaily_count,by=c('crossed_zcta','Date'),all.x=T)
```

#### Filter queries
```{r}
outcome_match<-read.csv("Z:\\Balaji\\SyS data\\sys_merged_outcomes.csv")
outcomes_all<-c('Pregnancy_complic', 'Asthma', 'Bite.Insect', 
                'Dehydration', 'Drowning', 'Hypothermia', 'Chest_pain', 
                'Heat_Related_But_Not_dehydration', 
                'CO_Exposure','Diarrhea','RespiratorySyndrome')
sys_sa<-merge(sys_sa,outcome_match[,c('row_id',outcomes_all)],all.x=T,by='row_id')

#creat a collective column
sys_sa$outcomes_any<-sys_sa[,outcomes_all] %>% head %>% apply(1,FUN=any)

#remove duplicate dfs
remove(sys_raw)
remove(outcome_match)
remove(ZCTAdaily_count)
remove(zip_zcta_crosswalk)
#make a copy of sys_sa
sys_sa_bkp<-sys_sa
remove(sys_sa)
```
 -->


## ______Do not change any thing in `sys_sa_bkp` after this point ___________

#### Load packages
```{r , message=FALSE}
library(GISTools)
library(rgdal)
library(dplyr)
library(plotly)
library(zoo)
#library(zipcodeR)
library(ggplot2)
library(sf)
library(readxl)
library(plyr)
library(knitr)
library(hrbrthemes)
```
#### Reload from bkp and remove non-complete rows and reduce race categories
```{r}
sys_sa<-subset(sys_sa_bkp,select=-c(zip_5c, Time, Zipcode, HospitalName, HospitalZipCode))

#reduce race categories [combine American Indian and Hawaiian into others]
sys_sa$Race<-recode(sys_sa$Race,`American Indian`='Others',Hawaiian='Others')
#remove incomplete recs
sys_sa<-sys_sa[complete.cases(sys_sa),]
#check if all columns are cmplete
#colSums(is.na(sys_sa))
```

#### Threshould for creating binary flood variable
```{r}
sys_sa$flooded <- cut(sys_sa$ZCT_f_R,c(0,quantile(inun_zcta$ZCT_f_R[inun_zcta$ZCT_f_R>0],
                      probs = seq(0,1,1/2))),include.lowest = T,right = F)
labels <- c('Non flooded','moderately flooded','highly flooded')
"Categories and intervals:";labels; levels(sys_sa$flooded)
levels(sys_sa$flooded)<-labels
```


#### Define flood period and control period
```{r}
sys_sa$period<-'controlPeriod'
sys_sa$period[sys_sa$Date>='2019-09-12' & sys_sa$Date<='2019-09-18'] <-'washoutPeriod'
sys_sa$period[sys_sa$Date>='2019-09-19' & sys_sa$Date<='2019-10-01'] <-'floodPeriod'
sys_sa$period[sys_sa$Date>='2019-10-02' & sys_sa$Date<='2019-11-01'] <-'monthAfterFlood'
sys_sa$period[sys_sa$Date>='2019-11-02' & sys_sa$Date<='2019-12-31'] <-'novAndDec'

#remove washout period
sys_sa<-subset(sys_sa,period!='washoutPeriod')

#sys_sa$period[sys_sa$Date<='2019-07-11'] <-'x_july_flood'
sys_sa$period<-factor(sys_sa$period)
levels(sys_sa$period)
```

####prepare lag variables for gee model
```{r}
library(dlnm)
#order the df by date
sys_sa<-sys_sa[order(sys_sa$Date),]
#create date and expoure
lag_df<-sys_sa[!duplicated(sys_sa$Date),c('Date','period')]
lag_df<-lag_df[order(lag_df$Date),]
row.names(lag_df) <- NULL
#define the peiod of exposure
lag_df$period[lag_df$Date>'2019-09-22']<-'controlPeriod'
lag_df$period<-droplevels(lag_df$period)
levels(lag_df$period)<-c(0,1)
#run cross basis
cb<-crossbasis(lag_df$period,lag=40,argvar=list(fun="strata",breaks=1),
  arglag=list(fun="strata",breaks=c(1,9)))
#bind date to the lag df and drop the binary exposure and keep lag
lag_df<-cbind(lag_df,cb)
lag_df<-subset(lag_df,select=-period)
#merge this back to df
sys_sa<-merge(sys_sa,lag_df,by='Date',all.x=T)

#exptract the lag _df seperately
lag_df_all<-sys_sa[,colnames(cb)]
#set expsoure variable to 0 for non flooded tracts? 
#lag_df_all[sys_sa$flooded=='Non flooded',]<-0
#make the lag df into cross basis object
lag_df_all<-as.matrix(lag_df_all)
class(lag_df_all) <- c("crossbasis","matrix")
for(i in c('arglag','df',"range","lag","argvar")){
attr(lag_df_all,i)<-attr(cb,i)
}

lag_cols<-colnames(cb)
```

#### python gee glm
```{r}
#configure reticulate
library(reticulate)
reticulate::use_condaenv(conda='C:\\Users\\balajiramesh\\Anaconda3\\condabin\\conda.bat',required = T,condaenv = 'r-reticulate')
```

```{python}
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import numpy as np

#get the data and colnames of lag variables from R
sys_sa= r.sys_sa.copy()
lag_cols=pd.Series(r.lag_cols)
#change column names
old_names=sys_sa.columns[sys_sa.columns.str.contains("\.")]
new_names=old_names.str.replace("\.","_")
sys_sa.rename(columns=dict(zip(old_names, new_names)),inplace=True)

#define outcome
outcome='Pregnancy_complic'
#run gee glm
formula=outcome+'.astype(float) ~ '+ ' + '.join(lag_cols.str.replace('\.','_') + ' * flooded') + ' + Ethnicity + Race + Sex + Age + weekday' 
model = smf.gee(formula=formula,groups=sys_sa.crossed_zcta, data = sys_sa,offset=np.log(sys_sa.ZCTAdaily_count),missing='drop',family=sm.families.Poisson(link=sm.families.links.log()))

results=model.fit()
#print(results.summary())
print('completed')
```
#### pull the results from the model and extract the need params and coeff
```{r}
gee_coeff<-data.frame(py$results$params)
gee_cov<-py$results$cov_params_default


lag_cols<-gsub('\\.','_',lag_cols)
#rownames(gee_coeff)[grep(paste0(lag_cols,':.*flooded',collapse = '|'),rownames(gee_coeff))]

to_filter<-'highly flooded'
indxs<-grep(paste0(lag_cols,':.*',to_filter,collapse = '|'),rownames(gee_coeff))
gee_cov[indxs,indxs]



pred7 <- crosspred(lag_df_all,coef =  gee_coeff[indxs,],vcov =gee_cov[indxs,indxs],model.link = 'log',at=1)

tablag2 <- with(pred7,t(rbind(matRRfit,matRRlow,matRRhigh)))
colnames(tablag2) <- c("RR","ci.low","ci.hi")
tablag2
tablag2[c(1,2,10),]
```


#### Outcome to analyse
```{r}
outcome<-'Pregnancy_complic'
sys_sa$outcome<-as.numeric(sys_sa[,outcome])
```

### Run models
#### Glm with binary outcomes
```{r}
start.time <- Sys.time()
model <-glm(formula = outcome ~ flooded * period + Age + Sex + Ethnicity + Race + weekday,
                    data = sys_sa,#id=crossed_zcta,
                    family = poisson(link='log'),
                    #corstr = "ar1", 
                    offset=log(ZCTAdaily_count))
summary(model)
"\n Time taken:"; Sys.time()-start.time
```

#### sample dataa
```{r}
sam_data<-sys_sa#sample_n(sys_sa,100000)
sam_data$ids<-as.numeric(as.factor(sam_data$crossed_zcta),levels=seq(1,length(unique(sam_data$crossed_zcta))))
sam_data<-sam_data[order(sam_data$ids),c("outcome" ,"flooded", "period", "Age", "Sex","Ethnicity","Race","weekday", "crossed_zcta","ZCTAdaily_count","Date","ids")]
summary(sam_data)
```

#### gee using gee library
```{r}
library(gee)
start.time <- Sys.time()
model <-gee(formula = outcome ~ flooded * period + Age + Sex + Ethnicity + weekday + Race + offset(log(ZCTAdaily_count)),
                    data = sam_data,
                    id=ids,
                    family = poisson(link = 'log'), corstr = "exchangeable", 
                    silent=F, maxiter = 30,
                    #corstr = "ar1", 
                    #offset=log(ZCTAdaily_count)
            )
model
"\n Time taken:"; Sys.time()-start.time
```


#### geeglm model try
```{r}
library(geepack)
start.time <- Sys.time()
model <-geeglm(formula = outcome ~ flooded * period + Age + Sex + Ethnicity + weekday, # + Race,
                    data = sam_data,
                    id=ids,
                    family = poisson(link = 'log'), corstr = "exchangeable", 
                    offset=log(ZCTAdaily_count)
               )
summary(model)
"\n Time taken:"; Sys.time()-start.time
```


```{r}
##update correlation structure
usmodelnew2 <- update(usmodelnew, corstr = "exch")
usmodelnew3 <- update(usmodelnew, corstr = "independence")

##select correlation structure based on smallest QIC
model.sel(usmodelnew, usmodelnew2, usmodelnew3, rank=QIC)
usmodel.select<- usmodelnew

##get summary from the selected model
QIC(usmodelnew)
summary(usmodelnew)

# 'tidy' function to clean results from 'broom' package. 
tidy(usmodelnew, conf.int = TRUE)



sys_sa_subset<-sys_sa[,c("outcome","flooded","period","Age","Sex","Race","Ethnicity","ZCTAdaily_count",'crossed_zcta')]
#remove incomplete rows
sys_sa_subset<-na.omit(sys_sa_subset)

#run model
model <-geeglm(formula = outcome ~ flooded * period + Age + Sex + Race + Ethnicity,
            data = sys_sa_subset,id=crossed_zcta,
            family = poisson(link='log'),
            #corstr = "ar1", 
            offset=log(ZCTAdaily_count))
print(summary(model))
results<-data.frame(exp((summary(model)[["coefficients"]])[,'Estimate']))
results<-cbind(results,data.frame(exp(confint.default(model))))
colnames(results)<-c('estimate','conf_int2_5','conf_int_97_5')
View(results)
write.table(results, "clipboard", sep="\t", row.names=T,col.names = F)
print(outcome)

```

##_______________________Other modules for ploting and trial _____________________
####glm model with less dates for checkin lag models
```{r}
sam_data<-sam_data[(sam_data$Date<'2019-10-05') & (sam_data$Date>'2019-08-30'),]
sam_data$period<-droplevels(sam_data$period)
#bulild lag variable
library(dlnm)
sam_data<-sam_data[order(sam_data$Date),]
#create date and expoure
lag_df<-sam_data[!duplicated(sam_data$Date),c('Date','period')]
lag_df<-lag_df[order(lag_df$Date),]
row.names(lag_df) <- NULL
lag_df$period[lag_df$Date>'2019-09-22']<-'controlPeriod'
lag_df$period<-droplevels(lag_df$period)
levels(lag_df$period)<-c(0,1)
#run cross basis
cb<-crossbasis(lag_df$period,lag=10,argvar=list(fun="strata",breaks=1),
  arglag=list(fun="strata",breaks=c(4,8)))
#bind date to the lag df and drop the binary exposure and keep lag
lag_df<-cbind(lag_df,cb)
lag_df<-subset(lag_df,select=-period)
#merge this back to df
sam_data<-merge(sam_data,lag_df,by='Date',all.x=T)

#run glm 
lag_df_all<-sam_data[,colnames(cb)]
#lag_df_all[sam_data$flooded=='Non flooded',]<-0
lag_df_all<-as.matrix(lag_df_all)

class(lag_df_all) <- c("crossbasis","matrix")
for(i in c('arglag','df',"range","lag","argvar")){
attr(lag_df_all,i)<-attr(cb,i)
}


start.time <- Sys.time() 
model1 <-glm(formula = outcome ~ v1.l1 * flooded + v1.l2 * flooded + v1.l3 * flooded + Age + Sex + Ethnicity + Race + weekday,
                    data = sam_data,#id=crossed_zcta,
                    family = poisson(link='log'),
                    offset=log(ZCTAdaily_count))
summary(model2)
"\n Time taken:"; Sys.time()-start.time

pred7 <- crosspred(lag_df_all,coef = coef(model)[2:4],vcov = vcov(model)[2:4,2:4],model.link = 'log',at=1)

tablag2 <- with(pred7,t(rbind(matRRfit,matRRlow,matRRhigh)))
colnames(tablag2) <- c("RR","ci.low","ci.hi")
tablag2

sub_sam_data<-sample_n(sys_sa,10000)


```



####group by date and zctas and run gee and check corresponding result
```{r}
library(fastDummies)
grouped_sys<- subset(sam_data,select=-c(Race))
grouped_sys<-dummy_cols(grouped_sys,select_columns = c('Sex','Ethnicity'),remove_selected_columns = T)
#colnames(grouped_sys)
# group by date and zctas
grouped_sys<-grouped_sys %>% group_by(Date,crossed_zcta) %>% 
                                dplyr::summarise(outcome=sum(outcome),
                                flooded=first(flooded),
                                period=first(period),
                                Age=mean(Age),
                                weekday=first(weekday),
                                ZCTAdaily_count=first(ZCTAdaily_count),
                                Sex_F=sum(Sex_F),
                                Sex_M=sum(Sex_M),
                                Ethnicity_NON_HISPANIC=sum(`Ethnicity_NON HISPANIC`),
                                Sex_Unknown=sum(Sex_Unknown),
                                Ethnicity_HISPANIC=sum(Ethnicity_HISPANIC),
                                Ethnicity_Unknown=sum(Ethnicity_Unknown))

grouped_sys[,c('Sex_F','Sex_M','Sex_Unknown', 'Ethnicity_HISPANIC','Ethnicity_Unknown','Ethnicity_NON_HISPANIC')]<-apply(grouped_sys[,c('Sex_F','Sex_M','Sex_Unknown', 'Ethnicity_HISPANIC','Ethnicity_Unknown','Ethnicity_NON_HISPANIC')],2,as.numeric)
grouped_sys[,c('Sex_F','Sex_M','Sex_Unknown', 'Ethnicity_HISPANIC','Ethnicity_Unknown','Ethnicity_NON_HISPANIC')]<-grouped_sys[,c('Sex_F','Sex_M','Sex_Unknown', 'Ethnicity_HISPANIC','Ethnicity_Unknown','Ethnicity_NON_HISPANIC')]/grouped_sys$ZCTAdaily_count
#grouped_sys

#run model
library(geepack)
start.time <- Sys.time()
group_model <-geeglm(formula = outcome ~ flooded * period + Age + Sex_F + Sex_Unknown + Ethnicity_HISPANIC + Ethnicity_Unknown  + weekday, # + Race,
                    data = grouped_sys,
                    id=crossed_zcta,
                    family = poisson(link = 'log'), corstr = "independence", 
                    offset=log(ZCTAdaily_count)
               )
summary(group_model)
"\n Time taken:"; Sys.time()-start.time
```
#### Group by date and plot between flooded and non flooded
```{r}

grouped<-sys_sa %>% group_by(Date,flooded) %>% dplyr::summarise(count=sum(Pregnancy_complic))
grouped<-grouped[order(grouped$flooded, grouped$Date),]
grouped$count<-as.numeric(grouped$count)

#min max scale
normalize <- function(x){return((x- min(x)) /(max(x)-min(x)))}
for(i in levels(grouped$flooded)){
grouped[grouped$flooded==i,'count']=normalize(grouped[grouped$flooded==i,'count'])}

#rolling window of 7
grouped$rolled_count<-rollmean(grouped$count,7,fill=NA)
#plot 
fig<-plot_ly(x = grouped$Date, y = grouped$rolled_count, mode='lines',color=factor(grouped$flooded))
fig
```

#### plot stream gauges with grouped Sys with flooded and non-flooded
```{r warning=FALSE}
library(scales)
data<-read.csv("Z:\\Balaji\\stram_flow\\imelda\\data_gagues_ZCTA_study_area.csv")
data$datetime<-data$datetime_Converted
grouped_fg<-data %>% 
  group_by(datetime) %>% 
  dplyr::summarise(above_floo = sum(na.omit(exceed_flood_stage)))
grouped_fg$date<-as.Date(grouped_fg$datetime,format='%Y-%m-%d')
grouped_fg<-grouped_fg[order(grouped_fg$datetime),]

#merge sys_groupd counts
grouped_fg=merge(grouped,grouped_fg,by.x='Date',by.y='date', all.x=T)

#plot
#blind frinedly pallete
cbbPalette <- c( "#009E73","#0072B2", "#D55E00", "#CC79A7", "#000000", "#E69F00", "#56B4E9", "#F0E442")
# To use for fills, add
scale_fac=1500
scale_fill_manual(values=cbbPalette)
ggplot(grouped_fg, aes(x=Date)) +
  
  geom_bar( aes(y=above_floo*scale_fac), stat="sum",size=.1, fill='#56B4E9',alpha=.6) + 
  geom_line( aes(y=rolled_count,colour=flooded), size=0.8) +
  xlab('Date') +
  scale_y_continuous(
    # Features of the first axis
    name = "ED visits per day",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~./scale_fac, name="Number of guages \nindicating flooding")
  ) + 
  scale_x_date(date_breaks = "15 day", expand = c(0,0),
                 labels=date_format("%d/%m/%y"),
                 limits = as.Date(c('2019-06-01','2019-12-31'))) +
  #theme_ipsum() +
  theme(
    text = element_text(size=14),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position="bottom",
    legend.title =element_blank()
  )+ scale_color_manual(values=cbbPalette) + ggtitle('Overall ED visits')
ggsave('SysWithGaugesAll.pdf',path=getwd(),height=10,width = 20,units = "cm")

```

### Run glm to see how to week and month can fit the data
```{r}
#create grouped df
grouped<-sys_sa %>% group_by(Date) %>% dplyr::summarise(count=n())
grouped$rolled_count<-rollmean(grouped$count,7,fill=NA)
#create daye variables
grouped$weekday<-as.factor(weekdays(grouped$Date,abbreviate=T))
grouped$month<-as.factor(months(grouped$Date,abbreviate = T))

plot(grouped$Date,grouped$count,col=grey(0.6),main="ED per day",ylab="ED visits", type='b',
  xlab="Date")

plot(grouped$Date,grouped$rolled_count,col=grey(0.6),main="ED per day (7 day avg)",ylab="ED visits", type='b',
  xlab="Date")
```

#### run glm fit using seasonality and 
```{r message=FALSE}
model1 <- glm(count ~ weekday  ,grouped,family=poisson)
summary(model1)

# COMPUTE PREDICTED NUMBER OF DEATHS FROM THIS MODEL
pred1 <- predict(model1,type="response")

plot(grouped$Date,grouped$count,col=grey(0.6), type='p',
  main="Time-stratified model (month strata)",ylab="Daily number of deaths",
  xlab="Date")
lines(grouped$Date,pred1,lwd=1)

model2 <- glm(count ~ weekday + month,grouped,family=poisson)
summary(model2)

# COMPUTE PREDICTED NUMBER OF DEATHS FROM THIS MODEL
pred2 <- predict(model2,type="response")

plot(grouped$Date,grouped$count,col=grey(0.6), type='p',
  main="Time-stratified model (inteaction)",ylab="Daily number of deaths",
  xlab="Date")
lines(grouped$Date,pred2,lwd=1)
```


```{r eval=FALSE, include=FALSE}
#### See number of hospitals broadcasting data
grouped_hosp<-sys_sa  %>% group_by(Date) %>% dplyr::summarise(count=n_distinct(HospitalName))
grouped_hosp$rolled_count<-rollmean(grouped_hosp$count,7,fill=NA)
plot_ly(x = grouped_hosp$Date, y = grouped_hosp$rolled_count, mode='lines')
```
### Counts of diffent outcomes over each flood group
```{r}
grouped_outcomes<-sys_sa %>% group_by(Date,flooded) %>% dplyr::summarise(
                                     Pregnancy_complic=sum(Pregnancy_complic),
                                     Asthma=sum(Asthma),
                                     Bite.Insect=sum(Bite.Insect),
                                     Dehydration=sum(Dehydration),
                                     Drowning=sum(Drowning),
                                     Hypothermia=sum(Hypothermia),
                                     Chest_pain=sum(Chest_pain),
                                     Heat_Related_But_Not_dehydration=sum(Heat_Related_But_Not_dehydration),
                                     CO_Exposure=sum(CO_Exposure)
                                     )
grouped_outcomes<-grouped_outcomes[order(grouped$flooded, grouped$Date),]
#create pivot
library(tidyr)
pivot<-grouped_outcomes %>% pivot_wider(names_from = flooded, values_from=outcomes_all)
write.csv(pivot,'SysOutcomesDailyCounts.csv',row.names = F)
```
